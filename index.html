---
layout: page
title: About Me
description: "Hi, I'm Chi Zhang (张驰)."
header-img: "img/banner/about-banner.jpg"
---

<div class="post-container">

  <blockquote>
    <p>"The study of vision must therefore include not only the study of how to extract from images the various aspects
      of the world that are useful to us, but also an inquiry into the nature of the <strong>internal
        reprensentations</strong> by which we capture this information and thus make it available as a
      <strong>basis</strong> for decisions about our thoughts and actions." &#8208 David Marr
    </p>
  </blockquote>
  <!-- <p>
    <font color="red">News!</font> My PhD finale accepted in Science Advances.
  <p>
    <font color="red">News!</font> One paper accepted in ICLR 2024.
  <p>
    <font color="red">News!</font> Three papers accepted in NeurIPS 2023 with one Spotlight.
  <p>
    <font color="red">News!</font> One paper accepted as Oral in ICCV 2023.
  <p>
    <font color="red">News!</font> One paper accepted in ICML 2023.
  <p>
    <font color="red">News!</font> One paper accepted as Spotlight in NeurIPS 2022.
  <p>
    <font color="red">News!</font> One paper accepted in ECCV 2022.
  <p>
    <font color="red">News!</font> Two papers accepted in CVPR 2021.
  <p>
    <font color="red">News!</font> One paper accpeted in ICRA 2021.
  <p>
    <font color="red">News!</font> Our position paper featured in Engineering.
  <p>
    <font color="red">News!</font> One paper accepted as Oral in AAAI 2020.
  <p>
    <font color="red">News!</font> One paper accepted as Spotlight in NeurIPS 2019.
  <p>
    <font color="red">News!</font> One paper accepted in IROS 2019.
  <p>
    <font color="red">News!</font> One paper accepted in CVPR 2019.
  <p>
    <font color="red">News!</font> Two papers accepted in AAAI 2019.
  </p> -->

  <h2 id="about-me">About Me</h2>

  <p>I'm Chi Zhang (张驰). I recently joined <a href="https://seed.bytedance.com/en/">ByteDance Seed</a> after a few wonderful years at <a href="https://www.bigai.ai/">Beijing Institute for General Artificial Intelligence (BIGAI)</a>, as a full-time research scientist. Before that, I obtained my Ph.D. degree from <em>Department of Computer Science, University of California - Los Angeles</em>, advised by <a href="http://www.stat.ucla.edu/~sczhu/">Professor Song-Chun Zhu</a>.</p>

  <p>My recent reseach interests include <em>abstract reasoning</em>, <em>active reasoning</em> and <em>applied research
      for challenging problems</em>.</p>

  <p>I used to work with <a href="https://home.cse.ust.hk/~dyyeung/www/Home.html">Professor Dit-Yan Yeung</a> at
    <em>Hong Kong University of Science and Technology</em> and <a
      href="http://www.cad.zju.edu.cn/home/dengcai/">Professor Deng Cai</a> at my alma mater, <em>Zhejiang
      University</em>.
  </p>

  <p>I serve as</p>
  <ul>
    <li>Conference reviewer: CVPR (from 2019), ICCV (from 2019), ECCV (from 2020), NeurIPS (from 2020), ICLR (from
      2020), ICML (from 2021), AAAI (from 2020)</li>
    <li>Journal reviewer: IEEE Transactions on Image Processing (TIP), Psychological Review</li>
  </ul>

  <p><a href="/attach/CV_eng_blog.pdf">CV</a> / <a href="mailto:wellyzhangc@gmail.com">Email</a> / <a
      href="https://github.com/WellyZhang">GitHub</a> </p>

  <h2 id="publications">Publications</h2>

  <div class="pub-img">
    <ul>
      <li>
        <img
          src="./attach/tonggeometry_mickymouse.png"
          class="thumbnail">
        <div class="title">Proposing and solving olympiad geometry with guided tree search</div>
        <div class="authors"><strong>Chi Zhang</strong>, Jiajun Song, Siyu Li, Yitao Liang, Yuxi Ma, Wei Wang, Yixin Zhu, Song-Chun Zhu</div>
        <!-- <div class="venue"><strong>Proceedings of Annual Meeting of the Cognitive Science Society (CogSci), 2025</strong></div> -->
        <div class="resources">
          <a href="https://arxiv.org/abs/2412.10673">Paper</a> /
          <a href="https://mp.weixin.qq.com/s/eswmr-dcYx_oeYrKJwX0qQ">量子位</a>
        </div>
      </li>
      <hr>
      <li>
        <img
          src="https://yzhu.io/publication/intuitive2025cogsci/featured_hu3671077991454892291.webp"
          class="thumbnail">
        <div class="title">A simulation-heuristics dual-process model for intuitive physics</div>
        <div class="authors">Shiqian Li, Yuxi Ma, Jiajun Yan, Bo Dai, Yujia Peng, <strong>Chi Zhang</strong>, Yixin Zhu</div>
        <div class="venue"><strong>CogSci 2025</strong></div>
        <div class="resources">
          <a href="https://yzhu.io/publication/intuitive2025cogsci/paper.pdf">Paper</a> /
          <a href="https://github.com/lishiqianhugh/DualModel">Code</a> /
          <a href="https://vimeo.com/1073941780">Video</a> /
          <a href="https://yzhu.io/publication/intuitive2025cogsci/poster.pdf">Poster</a> /
          <a href="https://drive.google.com/file/d/1R6x_3L0DQFJ3HU0yYvjkhPGKA94tzNok/view?usp=sharing">Dataset</a>
        </div>
      </li>
      <hr>
      <li>
        <img
          src="https://yzhu.io/publication/creativity2025cogsci/featured_hu11322660183334191191.webp"
          class="thumbnail">
        <div class="title">Probing and Inducing Combinational Creativity in Vision-Language Models</div>
        <div class="authors">Yongqian Peng, Yuxi Ma, Mengmeng Wang, Yuxuan Wang, Yizhou Wang, <strong>Chi Zhang</strong>, Yixin Zhu, Zilong Zheng</div>
        <div class="venue"><strong>CogSci 2025</strong></div>
        <div class="resources">
          <a href="https://yzhu.io/publication/creativity2025cogsci/paper.pdf">Paper</a> /
          <a href="https://github.com/PPYYQQ/aicc-code">Code</a> /
          <a href="https://vimeo.com/1075960292">Video</a> /
          <a href="https://drive.google.com/drive/folders/1XIvOVwP0eVX60L-STugt_vi19Q_kAEMW">Dataset</a> /
          <a href="https://ppyyqq.github.io/aicc/">Project</a> /
          <a href="https://mp.weixin.qq.com/s/VXfFfKA6VBl7TmBYB8-XZA">北大AI院</a> /
          <a href="https://mp.weixin.qq.com/s/ZkkVx3TXUIiwe-XNyyWj3Q">量子位</a>
        </div>
      </li>
      <hr>
      <li>
        <img
          src="./img/about/papers/sciadv2024zhang.jpg"
          class="thumbnail">
        <div class="title">Human-level few-shot concept induction through minimax entropy learning</div>
        <div class="authors"><strong>Chi Zhang</strong>, Baoxiong Jia, Yixin Zhu, Song-Chun Zhu</div>
        <div class="venue"><strong>Science Advances</strong></div>
        <div class="resources">
          <a href="https://www.science.org/doi/10.1126/sciadv.adg2488">Paper</a> /
          <a href="http://paper.people.com.cn/rmrb/html/2024-04/29/nw.D110000renmrb_20240429_4-19.htm">人民日报</a> /
          <a href="https://app.xinhuanet.com/news/article.html?articleId=0804cceb6ad31b2f4f05fd261cc3946a&timestamp=7541">新华网</a> /
          <a href="https://digital.gmw.cn/2024-04/24/content_37283023.htm">光明网</a> /
          <a href="http://digitalpaper.stdaily.com/http_www.kjrb.com/kjrb/html/2024-05/20/content_571770.htm?div=-1">科技日报</a>
        </div>
      </li>
      <hr>
      <li>
        <img
          src="https://lishiqianhugh.github.io/figures/IPHYRE.jpg"
          class="thumbnail">
        <div class="title">I-PHYRE: Interactive Physical Reasoning</div>
        <div class="authors">Shiqian Li, Kewen Wu, <strong>Chi Zhang</strong>,Yixin Zhu</div>
        <div class="venue"><strong>ICLR 2024</strong></div>
        <div class="resources">
          <a href="https://arxiv.org/abs/2312.03009">Paper</a> /
          <a href="https://github.com/lishiqianhugh/IPHYRE">Code</a> /
          <a href="https://vimeo.com/793260764/2f77f9d5cb">Video</a> /
          <a href="https://yzhu.io/publication/intuitive2024iclr/poster.pdf">Poster</a> /
          <a href="https://lishiqianhugh.github.io/IPHYRE">Project</a> /
          <a href="https://mp.weixin.qq.com/s/QRIgozzM4rYDfqel3Pme-Q">北大AI院</a>
        </div>
      </li>
      <hr>
      <li>
        <img
          src="./img/about/papers/neurips23active.webp"
          class="thumbnail">
        <div class="title">Active Reasoning in an Open-World Environment </div>
        <div class="authors">Manjie Xu, Guangyuan Jiang, Wei Liang, <strong>Chi Zhang</strong>,
          Yixin Zhu</div>
        <div class="venue"><strong>NeurIPS 2023</strong></div>
        <div class="resources">
          <a href="https://arxiv.org/abs/2311.02018">Paper</a> /
          <a href="https://github.com/mjtsu/Conan-Active-Reasoning">Code</a> /
          <a href="https://yzhu.io/publication/intent2023neurips/supp.pdf">Supp</a> /
          <a href="https://yzhu.io/publication/intent2023neurips/poster.pdf">Poster</a> /
          <a href="https://sites.google.com/view/conan-active-reasoning">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/mpi.png" class="thumbnail">
        <div class="title">Evaluating and Inducing Personality in Pre-trained Language Models </div>
        <div class="authors">Guangyuan Jiang<a data-toggle="tooltip" title="Equal contribution"><sup>*</sup></a>, Manjie
          Xu<a data-toggle="tooltip" title="Equal contribution"><sup>*</sup></a>, Song-Chun Zhu, Wenjuan Han,
          <strong>Chi Zhang</strong>,
          Yixin Zhu
        </div>
        <div class="venue"><strong>NeurIPS 2023</strong></div>
        <div class="venue">
          <font color="red">Spotlight</font>
        </div>
        <div class="resources">
          <a href="https://arxiv.org/abs/2206.07550">Paper</a> /
          <a href="https://yzhu.io/publication/llm2023neurips/supp.pdf">Supp</a> /
          <a href="https://yzhu.io/publication/llm2023neurips/poster.pdf">Poster</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/est.png" class="thumbnail">
        <div class="title">Interactive Visual Reasoning under Uncertainty </div>
        <div class="authors">Manjie Xu<a data-toggle="tooltip" title="Equal contribution"><sup>*</sup></a>, Guangyuan
          Jiang<a data-toggle="tooltip" title="Equal contribution"><sup>*</sup></a>, Wei Liang, <strong>Chi
            Zhang</strong>,
          Yixin Zhu</div>
        <div class="venue"><strong>NeurIPS 2023 Datasets and Benchmarks</strong></div>
        <div class="resources">
          <a href="https://yzhu.io/publication/blicket2023neurips/paper.pdf">Paper</a> /
          <a href="https://github.com/mjtsu/IVRE">Code</a> /
          <a href="https://yzhu.io/publication/blicket2023neurips/supp.pdf">Supp</a> /
          <a href="https://yzhu.io/publication/blicket2023neurips/poster.pdf">Poster</a> /
          <a href="https://sites.google.com/view/ivre">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img
          src="./img/about/papers/iccv23xvoe.webp"
          class="thumbnail">
        <div class="title">X-VoE: Measuring eXplanatory Violation of Expectation in Physical Events </div>
        <div class="authors">Bo Dai, Linge Wang, Baoxiong Jia, Zeyu Zhang, Song-Chun Zhu, <strong>Chi Zhang</strong>,
          Yixin Zhu</div>
        <div class="venue"><strong>ICCV 2023</strong></div>
        <div class="venue">
          <font color="red">Oral</font>
        </div>
        <div class="resources">
          <a href="https://arxiv.org/abs/2308.10441">Paper</a> /
          <a href="https://github.com/daibopku/X-VoE">Code</a> /
          <a href="https://yzhu.io/publication/intuitive2023iccv/supp.pdf">Supp</a> /
          <a href="https://yzhu.io/publication/intuitive2023iccv/poster.pdf">Poster</a> /
          <a href="https://huggingface.co/datasets/RuriSama/X-VoE/tree/main">Dataset</a> /
          <a href="https://sites.google.com/view/x-voe">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/jiang2023mewl.png" class="thumbnail">
        <div class="title">MEWL: Few-shot multimodal word learning with referential uncertainty </div>
        <div class="authors">Guangyuan Jiang, Xanjie Xu, Shiji Xin, Wei Liang, Yujia Peng, <strong>Chi Zhang</strong>,
          Yixin Zhu</div>
        <div class="venue"><strong>ICML 2023</strong></div>
        <div class="resources">
          <a href="https://arxiv.org/abs/2306.00503">Paper</a> /
          <a href="https://github.com/jianggy/MEWL">Code</a> /
          <a href="https://sites.google.com/view/mewl">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="https://lishiqianhugh.github.io/figures/LfID.jpg" class="thumbnail">
        <div class="title">On the Learning Mechanisms in Physical Reasoning </div>
        <div class="authors">Shiqian Li<a data-toggle="tooltip" title="Equal contribution"><sup>*</sup></a>, Kewen Wu<a
            data-toggle="tooltip" title="Equal contribution"><sup>*</sup></a>, <strong>Chi Zhang</strong>,
          Yixin Zhu</div>
        <div class="venue"><strong>NeurIPS 2022</strong></div>
        <div class="venue">
          <font color="red">Spotlight</font>
        </div>
        <div class="resources">
          <a href="https://arxiv.org/abs/2210.02075">Paper</a> /
          <a href="https://pku.ai/publication/intuitive2022neurips/poster.pdf">Poster</a> /
          <a href="https://github.com/lishiqianhugh/LfID">Code</a> /
          <a href="https://lishiqianhugh.github.io/LfID_Page">Project</a> /
          <a href="https://mp.weixin.qq.com/s/rZiRji2FtMIWsu2CU1LfzQ">Media: Institute for AI, PKU</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/arxivzhang_alans.png" class="thumbnail">
        <div class="title">Learning Algebraic Representation for Systematic Generalization in Abstract Reasoning</div>
        <div class="authors"><strong>Chi Zhang</strong><a data-toggle="tooltip"
            title="Equal contribution"><sup>*</sup></a>, Sirui Xie<a data-toggle="tooltip"
            title="Equal contribution"><sup>*</sup></a>, Baoxiong Jia<a data-toggle="tooltip"
            title="Equal contribution"><sup>*</sup></a>, Ying Nian Wu, Song-Chun Zhu, Yixin Zhu</div>
        <div class="venue"><strong>ECCV 2022</strong></div>
        <div class="resources">
          <a href="./attach/eccv22zhang_alans.pdf">Paper</a> /
          <a href="./attach/eccv22zhang_alans_supp.pdf">Supp</a> /
          <a href="./attach/eccv22zhang_alans_poster.pdf">Poster</a> /
          <a href="https://github.com/WellyZhang/ALANS">Code</a> /
          <a href="./project/alans.html#dataset">Dataset</a> /
          <a href="./blog/2022/07/17/ALANS">Blog</a> /
          <a href="./project/alans.html">Project</a> /
          <a href="https://mp.weixin.qq.com/s/8IaHoocWgI6TDrXfLWm1bg">Media: Institute for AI, PKU</a> /
          <a href="https://news.pku.edu.cn/jxky/42ce8a1b1a514e688b1d092d270bd7d3.htm">Media: PKU News</a> /
          <a href="https://m.weibo.cn/status/4799391653170046?wm=3333_2001&from=10C8093010&sourcetype=weixin">Media: PKU
            Weibo</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/cvpr22t4vzhang_detrpp.png" class="thumbnail">
        <div class="title">DETR++: Taming Your Multi-Scale Detection Transformer</div>
        <div class="authors"><strong>Chi Zhang</strong>, Lijuan Liu, Xiaoxue Zang, Frederick Liu, Hao Zhang, Xinying
          Song, Jindong Chen</div>
        <div class="venue"><strong>T4V: Transformers for Vision @ CVPR 2022. </strong></div>
        <div class="resources">
          <a href="./attach/cvpr22t4vzhang_detrpp.pdf">Paper</a> /
          <a href="./attach/cvpr22t4vzhang_detrpp_poster.pdf">Poster</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/cvpr21zhang_acre.png" class="thumbnail">
        <div class="title">ACRE: <u>A</u>bstract <u>C</u>ausal <u>RE</u>asoning Beyond Covariation</div>
        <div class="authors"><strong>Chi Zhang</strong>, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, Yixin Zhu</div>
        <div class="venue"><strong>CVPR 2021</strong></div>
        <div class="resources">
          <a href="./attach/cvpr21zhang_acre.pdf">Paper</a> /
          <a href="./attach/cvpr21zhang_acre_supp.pdf">Supp</a> /
          <a href="./attach/cvpr21zhang_acre_poster.pdf">Poster</a> /
          <a href="https://github.com/WellyZhang/ACRE">Code</a> /
          <a href="./project/acre.html#dataset">Dataset</a> /
          <a href="./blog/2021/03/19/ACRE">Blog</a> /
          <a href="./project/acre.html">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/cvpr21zhang_prae.png" class="thumbnail">
        <div class="title">Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution</div>
        <div class="authors"><strong>Chi Zhang</strong><a data-toggle="tooltip"
            title="Equal contribution"><sup>*</sup></a>, Baoxiong Jia<a data-toggle="tooltip"
            title="Equal contribution"><sup>*</sup></a>, Song-Chun Zhu, Yixin Zhu</div>
        <div class="venue"><strong>CVPR 2021</strong></div>
        <div class="resources">
          <a href="./attach/cvpr21zhang_prae.pdf">Paper</a> /
          <a href="./attach/cvpr21zhang_prae_supp.pdf">Supp</a> /
          <a href="./attach/cvpr21zhang_prae_poster.pdf">Poster</a> /
          <a href="https://github.com/WellyZhang/PrAE">Code</a> /
          <a href="./blog/2021/03/13/PrAE">Blog</a> /
          <a href="./project/prae.html">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/icra21xie.png" class="thumbnail">
        <div class="title">Congestion-aware Multi-agent Trajectory Prediction for Collision Avoidance</div>
        <div class="authors">Xu Xie, <strong>Chi Zhang</strong>, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu</div>
        <div class="venue"><strong>ICRA 2021</strong></div>
        <div class="resources">
          <a href="https://xuxie1031.github.io/projects/GTA/GTAResource/ICRA21_GTA_Trajectory_Prediction.pdf">Paper</a>
          /
          <a href="https://vimeo.com/525104854">Demo</a> /
          <a href="https://github.com/xuxie1031/CollisionFreeMultiAgentTrajectoryPrediciton">Code</a> /
          <a href="https://xuxie1031.github.io/projects/GTA/GTAProj.html">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/dark.png" class="thumbnail">
        <div class="title">Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense</div>
        <div class="authors">Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Edmonds, Hangxin Liu, Feng Gao,
          <strong>Chi Zhang</strong>, Siyuan Qi, Ying Nian Wu, Joshua B. Tenenbaum, Song-Chun Zhu
        </div>
        <div class="venue"><strong>Engineering, Volume 6, Issue 3</strong></div>
        <div class="resources">
          <a href="./attach/dark.pdf">Paper</a> /
          <a href="https://www.sciencedirect.com/science/article/pii/S2095809920300345">Link</a> /
          <a href="https://mp.weixin.qq.com/s/NR1rTjvAaNuFfhzQYNICKA">Media: Engineering</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/aaai2020zhang.png" class="thumbnail">
        <div class="title">Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational
          Reasoning</div>
        <div class="authors">Wenhe Zhang, <strong>Chi Zhang</strong>, Yixin Zhu, Song-Chun Zhu</div>
        <div class="venue"><strong>AAAI 2020</strong></div>
        <div class="venue">
          <font color="red">Oral</font>
        </div>
        <div class="resources">
          <a href="./attach/aaai20zhang.pdf">Paper</a> /
          <a href="./attach/aaai20zhang_poster.pdf">Poster</a> /
          <a href="https://github.com/zwh1999anne/Machine-Number-Sense-Dataset">Code</a> /
          <a href="https://drive.google.com/file/d/1k5G32U9dGsS2I-k7KlSfu9CCOZ6gF4Gh/view?usp=sharing">Dataset (Google Drive)</a> /
          <a href="https://pan.baidu.com/s/1IeoIqInbIp3ulznu2YNzYQ?pwd=mxkd">Dataset (BaiduNetdisk)</a> /
          <a href="https://sites.google.com/view/number-sense/home">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/neurips2019zhang.jpg" class="thumbnail">
        <div class="title">Learning Perceptual Inference by Contrasting</div>
        <div class="authors"><strong>Chi Zhang</strong><a data-toggle="tooltip"
            title="Equal contribution"><sup>*</sup></a>, Baoxiong Jia<a data-toggle="tooltip"
            title="Equal contribution"><sup>*</sup></a>, Feng Gao, Yixin Zhu, Hongjing Lu, Song-Chun Zhu</div>
        <div class="venue"><strong>NeurIPS 2019</strong></div>
        <div class="venue">
          <font color="red">Spotlight</font> (2.43% acceptance rate)
        </div>
        <div class="resources">
          <a href="./attach/neurips19zhang.pdf">Paper</a> /
          <a href="./attach/neurips19slides.pdf">Slides</a> /
          <a href="./attach/neurips19poster.pdf">Poster</a> /
          <a href="https://github.com/WellyZhang/CoPINet">Code</a> /
          <a href="./blog/2019/11/25/CoPINet">Blog</a> /
          <a href="./project/copinet.html">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="https://xuxie1031.github.io/resources/iros19.png" class="thumbnail">
        <div class="title">Learning Virtual Grasp with Failed Demonstrations via Bayesian Inverse Reinforcement Learning
        </div>
        <div class="authors">Xu Xie<a data-toggle="tooltip" title="Equal contribution"><sup>*</sup></a>, Changyang Li<a
            data-toggle="tooltip" title="Equal contribution"><sup>*</sup></a>, <strong>Chi Zhang</strong>, Yixin Zhu,
          Song-Chun Zhu</div>
        <div class="venue"><strong>IROS 2019</strong></div>
        <div class="resources">
          <a href="https://xuxie1031.github.io/resources/iros19xie.pdf">Paper</a> /
          <a href="https://vimeo.com/350872475">Demo</a> /
          <a href="https://github.com/xuxie1031/VRGraspIRLEnv">Code</a> /
          <a href="https://xuxie1031.github.io/projects/VRGrasp/VRGraspProj.html">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/cvpr19zhang.jpg" class="thumbnail">
        <div class="title">RAVEN: A Dataset for <u>R</u>elational and <u>A</u>nalogical <u>V</u>isual
          r<u>E</u>aso<u>N</u>ing</div>
        <div class="authors"><strong>Chi Zhang</strong><a data-toggle="tooltip"
            title="Equal contribution"><sup>*</sup></a>, Feng Gao<a data-toggle="tooltip"
            title="Equal contribution"><sup>*</sup></a>, Baoxiong Jia, Yixin Zhu, Song-Chun Zhu</div>
        <div class="venue"><strong>CVPR 2019</strong></div>
        <div class="resources">
          <a href="./attach/cvpr19zhang.pdf">Paper</a> /
          <a href="./attach/cvpr19zhang_supp.pdf">Supp</a> /
          <a href="./attach/cvpr19poster.pdf">Poster</a> /
          <a href="https://github.com/WellyZhang/RAVEN">Code</a> /
          <a href="./project/raven.html#dataset">Dataset</a> /
          <a href="./blog/2019/03/07/RAVEN">Blog</a> /
          <a href="./project/raven.html">Project</a> /
          <a href="https://www.jiqizhixin.com/articles/19031104">Media: Synced</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/aaai19zhang.png" class="thumbnail">
        <div class="title">MetaStyle: Three-Way Trade-Off Among Speed, Flexibility, and Quality in Neural Style Transfer
        </div>
        <div class="authors"><strong>Chi Zhang</strong>, Yixin Zhu, Song-Chun Zhu</div>
        <div class="venue"><strong>AAAI 2019</strong></div>
        <div class="resources">
          <a href="./attach/aaai19zhang.pdf">Paper</a> /
          <a href="./attach/aaai19zhang_supp.pdf">Supp</a> /
          <a href="./attach/aaai19slides.pdf">Slides</a> /
          <a href="./attach/aaai19poster.pdf">Poster</a> /
          <a href="https://vimeo.com/303954291">Demo</a> /
          <a href="https://github.com/WellyZhang/MetaStyle">Code</a> /
          <a href="./blog/2018/12/06/MetaStyle">Blog</a> /
          <a href="./project/metastyle.html">Project</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/aaai19liu.png" class="thumbnail">
        <div class="title">Mirroring without Overimitation: Learning Functionally Equivalent Manipulation Actions</div>
        <div class="authors">Hangxin Liu, <strong>Chi Zhang</strong>, Yixin Zhu, Chenfanfu Jiang, Song-Chun Zhu</div>
        <div class="venue"><strong>AAAI 2019</strong></div>
        <div class="resources">
          <a href="./attach/aaai19liu.pdf">Paper</a> /
          <a href="./blog/2018/12/04/mirroring">Blog</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/IJCAI18.png" class="thumbnail">
        <div class="title">Learning Unmanned Aerial Vehicle Control for Autonomous Target Following</div>
        <div class="authors">Siyi Li, Tianbo Liu, <strong>Chi Zhang</strong>, Dit-Yan Yeung, Shaojie Shen</div>
        <div class="venue"><strong>IJCAI 2018</strong></div>
        <div class="resources">
          <a href="https://www.ijcai.org/proceedings/2018/0685.pdf">Paper</a> /
          <a href="./blog/2018/11/26/quadrotor-tracking">Blog</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/Neurocomputing.png" class="thumbnail">
        <div class="title">Question Retrieval for Community-based Question Answering via Heterogeneous Social
          Influential Network</div>
        <div class="authors">Zheqian Chen, <strong>Chi Zhang</strong>, Zhou Zhao, Chengwei Yao, Deng Cai</div>
        <div class="venue"><strong>Neurocomputing, Volume 285</strong></div>
        <div class="resources">
          <a href="https://www.sciencedirect.com/science/article/pii/S0925231218300523">Paper</a> /
          <a href="./blog/2018/11/26/CQA">Blog</a>
        </div>
      </li>
      <hr>
      <li>
        <img src="./img/about/papers/Patent.png" class="thumbnail">
        <div class="title">A Method of Exact 3D Modeling Based on Natural Gestures via Data Gloves (in Chinese)</div>
        <div class="authors">Xiangdong Li, Sihong Lv, Yikun Wang, Xiaowo Sun, <strong>Chi Zhang</strong></div>
        <div class="venue"><strong>Patent publication number: CN104778746 B</strong></div>
        <div class="resources">
          <a href="https://www.google.com/patents/CN104778746B">Link</a>
        </div>
      </li>
    </ul>
  </div>

  <h2 id="experience">Education</h2>

  <div class="exp-proj">
    <ul>
      <li>
        <img src="./img/about/experience/UCLA_logo.png">
        <h4>Ph.D. in Computer Science</h4>
        <h4>
          <small>09.2017 - Summer 2022 | Los Angeles, California, USA</small>
        </h4>
        <h5>University of California, Los Angeles</h5>
      </li>
      <li>
        <img src="./img/about/experience/UCLA_logo.png">
        <h4>Master of Science in Computer Science</h4>
        <h4>
          <small>09.2017 - 03.2019 | Los Angeles, California, USA</small>
        </h4>
        <h5>University of California, Los Angeles</h5>
      </li>
      <li>
        <img src="./img/about/experience/ZJU_logo.png">
        <h4>Bachelor of Engineering in Computer Science</h4>
        <h4>
          <small>09.2013 - 06.2017 | Hangzhou, Zhejiang, China</small>
        </h4>
        <h5>Zhejiang University</h5>
      </li>
    </ul>
  </div>

  <!-- <h2 id="experience">Experience</h2>

  <div class="exp-proj">
    <ul>
      <li>
        <img src="./img/about/experience/Didi_logo.png">
        <h4>Machine Learning Engineer</h4>
        <h4>
          <small>Autonomous driving</small>
          <br>
          <small>04.2017 - 06.2017 | Hangzhou, Zhejiang, China</small>
        </h4>
        <h5>Didi Research Institute</h5>
      </li>
      <li>
        <img src="./img/about/experience/HKUST_logo.png">
        <h4>Research Intern</h4>
        <h4>
          <small>Target following with drones</small>
          <br>
          <small>09.2016 - 03.2017 | Clear Water Bay, Kowloon, Hong Kong</small>
        </h4>
        <h5>Hong Kong University of Science and Technology</h5>
      </li>
      <li>
        <img src="./img/about/experience/ZJU_logo.png">
        <h4>Research Assistant</h4>
        <h4>
          <small>Automatic number plate detection</small>
          <br>
          <small>03.2015 - 06.2016 | Hangzhou, Zhejiang, China</small>
        </h4>
        <h5>State Key Lab of CAD &amp; CG, Zhejiang University</h5>
      </li>
    </ul>
  </div> -->

  <!-- <h2 id="projects">Projects</h2>

  <div class="exp-proj">
    <ul>
      <li>
        <img src="./img/about/projects/mxnet_logo.png">
        <h4>MXNet</h4>
        <h4>
          <small>09.2016 - 04.2017</small>
        </h4>
      </li>
      <li>
        <img src="./img/about/projects/Darknet_logo.png">
        <h4>Automatic Number Plate Detection</h4>
        <h4>
          <small>03.2015 - 06.2016</small>
        </h4>
      </li>
    </ul>
  </div> -->

  <!-- <h2 id="presentations">Presentations</h2>

<ul>
  <li><a href="/attach/Detection_as_Regression.pdf">Detection as Regression</a></li>
  <li><a href="/attach/Post-ANPR.pdf">Real-Time Automatic Number Plate Detection</a></li>
</ul> -->

</div>